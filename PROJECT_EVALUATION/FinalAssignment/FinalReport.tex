\documentclass[11pt]{article}
\newcommand{\shellcmd}[1]{\\\indent\indent\texttt{\# #1}\\}
\newcommand{\mysqlcmd}[1]{\\\indent\indent\texttt{mysql> #1}\\}
\title{\textbf{MSWL Project Evaluation: OpenNebula Project Analysis}}
\author{Sergio Arroutbi Braojos}
\date{\today}

\usepackage{hyperref}
\usepackage[bottom=14em]{geometry}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{cite}
\usepackage{varioref}
\usepackage{placeins}
\usepackage{float}
\usepackage{graphicx}
\usepackage{booktabs}

\restylefloat{table}

\begin{document}

\hypersetup
{   
pdfborder={0 0 0}
}
   
\maketitle

\tableofcontents

\pagebreak

\section{Executive Summary}
This paper tries to perform an estimation of quality for a particular Cloud Computing Open Source Project, in this case, OpenNebula. Apart from that, a comparison of this project to other similar projects which are its competitors, such as CloudStack, Eucalyptus and OpenStack, is performed, considering the same quality model and how these projects score on it.\\
\\
In particular, by means of using a role play, the selection of a Cloud Computing System by a company offering cloud computing, storage and network services for other companies, from the perspective of a member of the Board of Directors of the company.\\
\\
By means of definition of a custom quality model, focused on the necessities of an invented role, OpenNebula project is analyzed, scored, and compared to the other projects. The final aim of this document is to take a decission of which is the best alternative to use, taking into account the most important factors in terms of quality that a Cloud Computing System must have for the invented role.

\section{Introduction} \label{sec:introduction}
This document is an approach to demonstrate the different issues and evaluations that can be taken into consideration in order to make a decission on the selection of an Open Source Project. A complete analysis of the quality of a particular project, in this case, OpenNebula, will be performed, by applying a self-defined quality model. 

\subsection{Document Objectives}
Apart from previously approach, a comparison with the score of other projects into the same quality model will be performed, so that decission making will be structured by following next steps:

\begin{itemize}\itemsep0pt
\item{Defining a role that justifies why some aspects are more or less important from a quality perspective, hence which factors are more valuable to consider in a quality model, in such way that allows to select or discard a particular project.}
\item{Defining a quality model that has been initially agreed and drafted between different students, that allows to obtain a quality score so that comparison between other Cloud Computing Open Source Projects can be performed.}
\item{Defining the different values in terms of scoring (weights) that all of the statements included in the quality model have, according to the role selected.}
\item{Analysing a particular Cloud Computing Open Source Project, OpenNebula in this case, by obtaining metrics of it and sharing with the rest of the students.}
\item{Calculating the final scoring of this particular project, OpenNebula, according to the weights decided according to the role.}
\item{And finally, by comparing the final score to the rest of the projects and justifying the selection of the "Winner Project".}
\end{itemize}

\subsection{Document Structure}
This document follows next structure, in order to accomplish the different phases that must be considered before making the final decission on the Cloud Computing Open Source Project selected:

\begin{itemize}\itemsep0pt
\item{\textbf{Introduction}}. This chapter describes the final aim of this document, how it is structured and the role being taken in order to take into consideration the most important factors to make the final decission on the Cloud Computing Open Source Project.
\item{\textbf{Methodology}}. On this chapter, the Quality Model will be described, justifying the selection of the different attributes to consider, the metrics derived from them and the weights applied taking into consideration the role selected.
\item{\textbf{Analysis}}. Analysis chapter will justify the application of the model, which tools will be used in order to obtain and analyze the different representative metrics and the data sources used to retrieve them. A Goal-Question-Metric (GQM) model will be implemented to describe how OpenNebula project will be analysed. Chapter final aim is to calculate OpenNebula score obtained by the application of the Quality Model by using the different tools and source data, all of them described on the previous sources.
\item{\textbf{Results}}. In parallel to this document, and analysis of other Cloud Computing Open Source Projects have been performed with the same quality model. However, on those documents the particular weights applied are related to what the other students invented roles have considered to be appropriate. In this and those documents the metrics obtained will be shared. This chapter will perform the score of the rest of the projects, taking into account the same weights that this document have taken into consideration. With OpenNebula and the other Cloud Computing Open Source Projects score, the final decission will be taken.
\item{\textbf{Conclussion}}. Conclussion chapter will justify if results and Open Source Project selected does really fit into requirements specified. It will also summarize the different aspects collected on this document.

\item{\textbf{Bibliography and References}}. This chapter collects the references and bibliography followed to perform this analysis.

\subsection{Introduction to the Quality Model}
This document considers a Quality Model agreed between some students and a professor in order to have a common starting point. In section~\nameref{sec:methodology} a complete description of the Quality Model will be performed. However, an introduction to it is appropriate to later justify the most important Quality Model Attributes that the role will consider.\\
\\
The Quality Model Main Attributes agreed are described below:
\begin{enumerate}\itemsep0pt
\item{\textbf{Efficiency}}. From the perspective of performance of the software. E.g: Do benchmarks of the product exist? 
\item{\textbf{Documentation}}. Quality of documentation of the project. E.g: Is the project documentation updated frequently?
\item{\textbf{Functionality}}. Quality of the project in terms of representative functionality. E.g: Does this project have a web browser to configure and administrate the resources under control?
\item{\textbf{Professional Support}}. Amount of companies providing professional support. E.g: How many companies provide professional support?
\item{\textbf{Community}}. Community Health as a way of measuring quality. E.g: Has the number of committers grown or decreased in the last year?
\end{enumerate}

With previous quality attributes brought up, now an invention of the role, a description of it and a categorization of the most important attributes from her/his perspective can be performed.

\subsection{Role Description} \label{sec:roledesc}
This document assumes a role play that allows to determine the more important factors from a quality perspective. In particular, the role of a member of the Board of Directors have been considered. So, the main question to answer is next one: \textbf{Which are the more relevant factors for this role?}\\
\\
To answer this question, a first assumption of the personal background of this role must be performed. This member has been part of the leadership group of an important Open Source Project.\\
\\
For this reason, his personal and professional background makes him aware of the \textbf{Importance of the Community} for an Open Source Project to succeed along the time.\\
\\
Apart from being aware of the Community, this role considers that \textbf{Documentation} is as well an important factor, as it helps to strengthen and consolidate Community itself.\\
\\
Meanwhile, role's professional background and experience also makes him/her to be aware of the \textbf{Importance of having professional support}.\\
\\
Last, but not least, the board member considers that \textbf{having a full featured functionality and good efficiency/performance is not as important as the previous factors}, as having a strong community and a good professional support can help on improving both attributes in a relatively short period of time.
\end{itemize}

\section{Methodology} \label{sec:methodology}
In order to calculate what is the quality of a project, first of all the Quality Model must be defined. It is not the objective of this chapter to describe on a very detailed basis the different Software Quality Models and its characteristics. However, a very brief description and classification of them is required in order to evaluate them and select the most appropriate, or, as well, the one that can inspire the Final Quality Model.\\
\\
There are many different type of Quality Models to evaluate Software Quality. Several classifications can be taken into consideration:

\subsection{Software Generic Quality Models}
There are Quality Models that have been designed to measure the Quality of the Software, or, at least, the process that is followed to produce software. Examples of this kind of Quality Models are:
\begin{itemize}\itemsep0pt
\item{ISO 9126}~\cite{ISO00, ISO01}.
This model defines a set of six super-attributes that are considered the more relevant from Software Quality perspective:
\begin{enumerate}
\item{Functionality}
\item{Reliability}
\item{Usability}
\item{Efficiency}
\item{Maintainability}
\item{Portability}
\end{enumerate}
Each of the super-attributes defines a collection of sub-attributes as well. A collection of 27 sub-attributes derived from the super-attributes are defined.
Taking into consideration all of the attributes and sub-attributes, a consideration on the quality of the Software of a particular project can be performed.
\item{CMMI}~\cite{CMMI00}.
Code Maturity Model Integration (CMMI) is applied more to the process of Software production rather than the Software itself. Defined in Carnegie Mellon University, this model claims that the modle can be used to guide process improvement across a project, division, or an entire organization.
It basically defines a set of levels that allow to identify the state of the development process up to day. The levels defined are as follow, taking into account that the higher the level is, the more quality the development process of the project has:
\end{itemize}

\begin{itemize}\itemsep0pt
\item{\textbf{Level 1. Initial.}} Processes are unpredictible and unmeasurable, and no reactive.
\item{\textbf{Level 2. Managed.}} Processes are characterized for projects, and often reactive.
\item{\textbf{Level 3. Defined.}} Processes are characterized for the organization and proactive. 
\item{\textbf{Level 4. Quantitatively Managed.}} Development Processes are Measured and Under Control.
\item{\textbf{Level 5. Optimizing.}} Focus on Process Improvement.
\end{itemize}
Both previously defined models, although being complete in terms of quality of the Software and its development process respectiveley, do not take into consideration additional aspects which are considered important only in the case of Open Source Software Projects. Examples of factors not beeing taken into consideration are, e.g.:
\begin{itemize}\itemsep0pt
\item{Community commitment}
\item{Community activity}
\item{What kind of licensing is used? (Copyleft/Permissive)}
\end{itemize}
Fortunately, there have been several approaches to define Quality Models which are more appropriate to be applied to an Open Source Project, and, which indeed, address previous factors, as well as other different ones that are also important to measure particular Quality factors related to this kind of projects. Next section defines some of the most commonly used models.

\subsection{Open Source Software Project Quality Models}\label{sec:oss_quality_models}

As previously defined, some Open Source Software Project Quality Models have been defined. There are two basic kind of models, taking into account the complexity of its implementation:

\begin{itemize}\itemsep0pt
\item{\textbf{Light-weight}}: This kind of Quality Models are easy to accomplish. Examples of this kind of Quality Models are OpenBRR and QSoS:
\begin{itemize}\itemsep0pt
\item{\textbf{OpenBRR}}~\cite{OPENBRR00}.
Open Business Readiness Rating Quality model defines a Spreadsheet with a set of super-attributes. Each of the super-attributes (named Categories in the model) proposed by the model are:
\begin{enumerate}\itemsep0pt
\item{\textbf{Functionality}}
\item{\textbf{Usability}}
\item{\textbf{Quality}}
\item{\textbf{Security}}
\item{\textbf{Performance}}
\item{\textbf{Scalability}}
\item{\textbf{Architecture}}
\item{\textbf{Support}}
\item{\textbf{Documentation}}
\item{\textbf{Adoption}}
\item{\textbf{Community}}
\item{\textbf{Professionalism}}
\end{enumerate}
Each of the Categories define a set of subcategories (Metrics) with a description of the metric itself and the specification of the scores that apply to that particular metric. The user of the spreadsheet can assign different weights to both the category and the subcategories, in order to achieve a final score for the Open Source Project under study.\\
\\
The model gives, in the end, a Final Score for each project, what allows to the evaluator the possibility of quickly determining which is the most appropriate project according to her/his priorities in terms of Quality.\\
\\
This model, although beeing very improvable, has the merit of being the most simple one among the ones studied. For this reason, our Quality Model will be derived from an even more simple version of this model.\\
\item{\textbf{QSOS}}~\cite{QSOS00}. Qualification and Selection of Opensource Software is a Quality Model, licensed under GNU Free Documentation License.\\
\\
The quality model defines what is named as the "QSOS Manifesto", which establishes a list of statements for the objectives that this Quality Model tries to define, that can be summarized in:
\begin{itemize}\itemsep0pt
\item{Analyzing needs and limitations in software adoption}
\item{Evaluating functional and technical requirements}
\item{Formalising a methodology}
\item{Results reusing}
\item{Providing a free method}
\end{itemize}
Based on this manifesto, QSOS defines an iterative process that consist of four independent steps:

\begin{itemize}\itemsep0pt
\item{\textbf{Definition}}. Define a frame of references, with software families, types of licenses, types of communities, etc.
\item{\textbf{Evaluation}}. An Evaluation sheet is defined to evaluate the functional coverage and the risks that are to be considered from both user and service provider perspectives.
\item{\textbf{Qualification}}. This step define filters that take into account needs and limitations on the 
specific context of the project.
\item{\textbf{Selection}}. Identify software meeting the needs and/or requirements. Compare sofware and target selection.
\end{itemize}
To summarize, this model, despite the fact that is a light-weight model, is, somehow, a more difficult model to implement compared to OpenBRR. Apart from that, OpenBRR, having the possibility to assign different weights makes OpenBRR a more flexible model, more malleable to the necessities and priorities of a particular role or a particular company.
\end{itemize}
\item{\textbf{Heavy-weight}}: This kind of Quality Models are complex and not easily implementable. A good example of this kind of project is QualOSS.
\begin{itemize}\itemsep0pt
\item{\textbf{QualOSS}}~\cite{QUALOSS00}: Quality of Open Source Software. Started on 2006 and funded mainly by the Eurpoean Union, this project aimed to fill the gap in the state of the art of the Quality Models for Open Source Project Evaluation.\\
\\
It is considered a \textbf{heavy-weight} Quality Model, due to the progress and change of methodology that supposed compared to the already existing Quality Models, described above.\\
\\
One of its first goals consisted of semi-full automation of the analysis of the projects and metrics obtaining, by retrieving them using tools such as FLOSSMetrics~\cite{FLOSSM00}, together with several scripts provided for optimizing this purpose.\\
\\
As this document will not use or even inspire on QualOSS, not very deep information will be provided. However, last but not least, methodology used by this Quality Model is exposed below:
\begin{itemize}\itemsep0pt
\item{\textbf{Initial Steps}}: Consisted of interviews with companies and identification of priorities for them when selecting an Open Source Project. One of the main priorities identified were the Community, and concepts such as \textbf{Community Side}.
\item{\textbf{Basic Quality Model}}: The Methodology followed a GQM (Goal - Question - Metric) basis, where each goal  is divided into several questions, and several metrics are defined for each question.
\item{\textbf{Community Side}}: The main quality attributes identified for the community side were \textbf{Size and Regeneration Adequacy}, on the one hand, as well as \textbf{Interactivity and Workload Adequacy} on the other hand. 
\item{\textbf{Metrics}}: The metrics retrieved should be as objective as possible. One of more metrics are matched with a given question, and always Metrics were result of an average among some data. 
\end{itemize}

\end{itemize}
\end{itemize}

\subsection{Details of the Selected Quality Model}\label{sec:quality_model}
However, a particular Quality Model is not mandatory to be applied "AS IS". Some variations, limitations, trims, extensions or further considerations can be performed, according to the requirements and priorities defined for a particular Open Source Project.\\
\\
This document does not pretend to be an extensive report or investigation on Quality, but rather an explanation on what a definition of a simple Quality Model for a particular type of Open Source Projects, in this case, related to an incipient and very active technology as Cloud Computing is, and how a particular project scores to that model.\\
\\
For previous reason, in this case, a very simple Quality Model has been developed, taking some of the attributes of OpenBRR on the one hand, and on the other hand implementing and extending other attributes that have been considered to be important for this kind of Cloud Computing Open Source Projects.

\subsubsection{Quality Model Description}
The model is inspired on OpenBRR, but with a set of Categories and metrics which differ from the original model. Apart from that, some concepts from ISO 9126~\cite{ISO00, ISO01} have been taken into account.\\
\\
The set of Categories agreed for the Quality Model is described below:
\begin{itemize}\itemsep0pt
\item{\textbf{Efficiency}}
\item{\textbf{Documentation}}
\item{\textbf{Functionality}}
\item{\textbf{Professional Support}}
\item{\textbf{Community}}
\end{itemize}

Besides this, each of the categories, have a set of questions, with answers that are, somehow, measurable. Apart from that, there are several scores applicable, depending on the metrics obtained. Next tables define, for each category, the metrics and scores applicable.

\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{3cm} | p{5cm} | p{3cm} | l | }
    \hline
    Metric & Description & Score Specification & Score \\
    \hline
    Billing System & Does The Cloud System provide a Billing System for User Accounting? & Yes, fully supported & 5 \\ \cline{3-3} \cline{4-4} 
    & & Yes, partially supported & 3 \\ \cline{3-3}\cline{4-4}
    & & No & 1 \\ 
    \hline
    Multi-Platform Support & Does The Cloud System work on top of different Operating Systems?? & Yes, more than two & 5 \\ \cline{3-3} \cline{4-4} 
    & & Yes, two & 3 \\ \cline{3-3}\cline{4-4}
    & & No & 1 \\
    \hline
    Administration Configuration System & Does the Cloud System provide a System (Web preferably) to help on Administration Tasks? & Yes, a web framework & 5 \\ \cline{3-3} \cline{4-4} 
    & & Yes, not web-based & 3 \\ \cline{3-3}\cline{4-4}
    & & No & 1 \\
    \hline
    i18n & Does The Cloud System provide multi-language support & Yes, more than 5 languages & 5 \\ \cline{3-3} \cline{4-4} 
    & & Yes, from 1 to 5 languages & 3 \\ \cline{3-3}\cline{4-4}
    & & No & 1 \\
    \hline
    Quota Management & Does The Cloud System provide a system for quota management & Yes, for computing, storage and networking & 5 \\ \cline{3-3} \cline{4-4} 
    & & Yes, but not for all functionalities & 3 \\ \cline{3-3}\cline{4-4}
    & & No & 1 \\
    \hline
    \end{tabular}
    \caption{Functionality}
    \label{tab:functionality}
  \end{center}
\end{table}

\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{3cm} | p{5cm} | p{3cm} | l | }
    \hline
    Metric & Description & Score Specification & Score \\
    \hline
    Performance or Benchmark Tests Available & This measures if there was any performance testing done and benchmarks published — typically in comparison to other equivalent solutions & Yes, with good results & 5 \\ \cline{3-3} \cline{4-4} 
    & & Yes & 3 \\ \cline{3-3}\cline{4-4}
    & & No & 1 \\ 
    \hline
    Performance Tuning and Configuration & Is there documentation or tool to help fine-tune the component for performance? & Yes, extensive & 5 \\ \cline{3-3} \cline{4-4} 
    & & Yes, some & 3 \\ \cline{3-3}\cline{4-4}
    & & No & 1 \\ 
    \hline
    \end{tabular}
    \caption{Efficiency}
    \label{tab:efficiency}
  \end{center}
\end{table}

\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{3cm} | p{5cm} | p{3cm} | l | }
    \hline
    Metric & Description & Score Specification & Score \\
    \hline
    Company Support & Which is the amount of companies providing support ? & More than one & 5 \\ \cline{3-3} \cline{4-4} 
    & & Just one & 3 \\ \cline{3-3}\cline{4-4}
    & & No one & 1 \\ 
    \hline
    \end{tabular}
    \caption{Support}
    \label{tab:support}
  \end{center}
\end{table}

\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{3cm} | p{5cm} | p{3cm} | l | }
    \hline
    Metric & Description & Score Specification & Score \\
    \hline
    Documentation Update & When was the last time that documentation was updated ? & Last week & 5 \\ \cline{3-3} \cline{4-4} 
    & & Last Month & 4 \\ \cline{3-3}\cline{4-4}
    & & Last Three Months & 3 \\ \cline{3-3}\cline{4-4}
    & & Last Year & 2 \\ \cline{3-3}\cline{4-4}
    & & More than one year ago & 1 \\ 
    \hline
    Number of contributors to documentation & Amount of people who contributed to documentation last year & Ten or more people & 5 \\ \cline{3-3} \cline{4-4} 
    & & Five to ten people & 4 \\ \cline{3-3}\cline{4-4}
    & & Two to Five people & 3 \\ \cline{3-3}\cline{4-4}
    & & One person & 2 \\ \cline{3-3}\cline{4-4}
    & & No person & 1 \\ 
    \hline
    \end{tabular}
    \caption{Documentation}
    \label{tab:documentation}
  \end{center}
\end{table}

\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{3cm} | p{5cm} | p{3cm} | l | }
    \hline
    Metric & Description & Score Specification & Score \\
    \hline
    Mean commits & Which was the mean number of commits per developer last month? & 20 or more commits / developer & 5 \\ \cline{3-3} \cline{4-4}
    & & 10 to 20 commits / developer & 4 \\ \cline{3-3}\cline{4-4}
    & & 5 to 10 commits / developer & 3 \\ \cline{3-3}\cline{4-4}
    & & 1 to 5 commits / developer & 2 \\ \cline{3-3}\cline{4-4}
    & & \textless 1 commit / developer & 1 \\ 
    \hline
    File Territoriality & Percentage of files of the total modified by a unique developer & \textless 5\% & 5 \\ \cline{3-3} \cline{4-4}
    & & 5\%-10\%  & 4 \\ \cline{3-3}\cline{4-4}
    & & 10\%-20\% & 3 \\ \cline{3-3}\cline{4-4}
    & & 20\%-50\% & 2 \\ \cline{3-3}\cline{4-4}
    & & \textgreater 50\% & 1 \\ 
    \hline
    Community Growth & \% of committers number increase/decrease in the last year, calculated as percent difference in the amount of people who commited changes on 2012 compared to 2013 & \textgreater50\% & 5 \\ \cline{3-3} \cline{4-4}
    & & 25\% to 50\% & 4 \\ \cline{3-3}\cline{4-4}
    & & 0\% to 25\%  & 3 \\ \cline{3-3}\cline{4-4}
    & & 0\% to -25\% & 2 \\ \cline{3-3}\cline{4-4}
    & & \textless-25\%       & 1 \\ 
    \hline
    \end{tabular}
    \caption{Community}
    \label{tab:community}
  \end{center}
\end{table}
The Quality Model Spreadsheet previously described can be downloaded at next repository:\\
\\
\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev.git}\\
\\
In particular, the Quality Model Spreadsheet can be downloaded on path "spreadsheet/BRR\_ProjEval\_2013.ods", or directly through next path:\\
\\
\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev/blob/master/spreadsheet/BRR_ProjEval_2013.ods}
\subsubsection{Weights}\label{sec:weights}
Apart from the Quality Model Question / Metrics, the spreadsheet selected, inspired by OpenBRR Quality Model, contain the concept of "weights". Weights are simply a percentage to prioritize those aspects that are important for the role on her/his concept of quality. \\
\\
On previous section~\nameref{sec:roledesc}, the priorities for the role selected was explained from a descriptive perspective. However, on this section, a quantitative specification will be provided.\\
\\
As stated before, the role is aware of the \textbf{Importance of the Community} for an Open Source Project to succeed along the time. Apart from that, this role considers that \textbf{Documentation} is the next factor in terms of importance, as it helps to strengthen and consolidate Community itself.\\
\\
Role's experience also makes the \textbf{Importance of having professional support} important enough to consider it next. Last, but not least, the board member considers that \textbf{having a full featured functionality and good efficiency/performance is not as important as the previous factors}.\\
\\
However, although having not high priority, considers that efficiency has more impact on quality than functionality does. \textbf{Regarding metrics, no special concern is communicated from the role, so that all the metrics for each category seems important equally}.\\
\\
According to the role's priorities, her/his preferences and concerns, the resulting table for Categories will be as the following one:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | l | l | }
    \hline
    Category & Weight \\
    \hline
    Community & 30\%\\
    Documentation & 25\% \\
    Support & 20\% \\
    Efficiency & 15\% \\
    Functionality & 10\% \\
    \hline
    \end{tabular}
    \caption{Weights}
    \label{tab:weights}
  \end{center}
\end{table}

Regarding metrics, no special consideration has been taken into. Depending on the number of metrics, proportionality will be applied, so that weight is proportional to all the metrics. 
\begin{itemize}\itemsep0pt
\item{For those categories having two metrics, 50\% weight will be applied to the metrics.}
\item{For those categories having three metrics, 33,33\% weight will be applied to the metrics.}
\item{And so on, and so forth.}
\end{itemize}

\subsubsection{Tools for Metrics Retrieval}

Once Quality Model has been explained, it is time for an explanation on how to carry out the different methods to obtain the metrics associated to the quality attributes defined. Tools used to obtain the metrics have been basically three:
\begin{enumerate}\itemsep0pt
\item{\textbf{OpenNebula web page}}. To obtain information related to \textbf{Functionality} and \textbf{Professional Support}.
\item{\textbf{Google}}. To obtain information related to benchmarks comparing Cloud Computing Systems and, in this manner, obtain metrics related to \textbf{Efficiency}.
\item{\textbf{cvsanaly}}~\cite{CVSANALY00}. This tool, written in Python, allows obtaining and analyzing a particular software among different kind of them (CVS, as well as Subversion and GIT) and stores analyzed information on a SQL database. This information can be later analyzed in order to measure, for example:
\begin{itemize}\itemsep0pt
\item{Number of commits in a period of time, or a a particular time or date}.
\item{Most active committers}.
\item{And, in general, all the information that can be retrieved from that particular Software Configuration Manager}.
\item{Territoriality of the project, by means of calculation of the different people modifying a particular file}.
\item{Type of files commited}.
\end{itemize}
\end{enumerate}
This kind of analysis fits perfectly to obtain metrics of the Quality Model associated to \textbf{Community}and \textbf{Documentation}, with metrics such as last update of documentation, mean number of commits per developer or files territoriality as percentage of files being touched by a unique developer.

\subsubsection{Data Sources}
Regarding sources of data used to obtain the metrics, next datasets have been used:
\begin{itemize}\itemsep0pt
\item{\textbf{Documentation associated to Key Features on OpenNebula project}~\cite{OPNEB00}}. Through this documentation, all the characteristics, functionalities and different features provided by the main component have been found out. It has been used to find out attributes having to do with \textbf{Functionality} category, such as if Billing System exist, if there is a configuration tool for administration, etc.
\item{\textbf{Documentation associated to Commercial Support}~\cite{OPNEB01}}. This documentation has helped on finding out attributes having to do with \textbf{Support} category, clarifying the amount of companies providing professional support.
\item{\textbf{Cloud Stack Benchmark comparison}~\cite{BENCH00}}. This documentation has helped on finding out attributes having to do with \textbf{Efficiency} category, clarifying if Benchmark studies exist.
\end{itemize}

\section{Analysis}\label{sec:analysis}

This section summarizes the score obtained by OpenNebula Open Source project on the previously proposed Quality Model. Each of the next subsections cover the application of the Quality Model, and how by using the tools described on the data sources, an score is obtained for each category. Later, a final score will be obtained by setting weights to each metric and each category, to obtain the final score for OpenNebula.\\
\\
Spreadsheet covering all this information, weights and scores, can be checked on next URL:\\
\\
\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev.git}\\
\\
In particular, the Quality Model Spreadsheet can be downloaded on path "spreadsheet/OpenNebula/OpenNebula\_BRR\_ProjEval\_2013.ods", or directly through next path:\\
\\
\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev/blob/master/spreadsheet/OpenNebula/OpenNebula\_BRR\_ProjEval\_2013.ods}

\subsection{Functionality}
This chapter focuses on Functionality category metrics. It must be remarked that, for this category, following parameters have been considered:
\begin{itemize}\itemsep0pt
\item{\textbf{Tools:}} This category has been filled out basically through information existing on OpenNebula Open Source Project Web. Just a web browser (Firefox in this case) has been used.
\item{\textbf{Data Sources:}} Key Feature chapter on OpenNebula Open Source Project~\cite{OPNEB00}.
\item{\textbf{Category Weight:}} As previously stated, for this category it has been considered a weight of 10\%.
\end{itemize}
Taking into account previous data, next table shows the final score of this category.
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{4cm} | p{3cm} | l | p{1.2cm} | p{1.2cm} | }
    \toprule
    \textbf{Metric} & \textbf{Score Obtained} & \textbf{Score} & \textbf{Metric Weight} & \textbf{Metric Score}\\
    \hline
    Billing System & Yes, fully supported & 5 & 20\% & 1\\
    \hline
    Multi-Platform Support & No & 1 & 20\% & 0.2\\
    \hline
    Administration Configuration System & Yes, a web framework & 5 & 20\% & 1\\
    \hline
    i18n & Yes, more than 5 languages & 5 & 20\% & 1\\
    \hline
    Quota Management & Yes, for computing, storage and networking & 5 & 20\% & 1\\
    \midrule
    \textbf{Unweighted rating} & \multicolumn{4}{c|}{\textbf{4.2}}\\
    \hline
    \textbf{Weighted rating (10\%)} & \multicolumn {4}{c|}{\textbf{0.42}}\\
    \bottomrule
    \end{tabular}
    \caption{Functionality Score}
    \label{tab:func_score}
  \end{center}
\end{table}

\subsection{Efficiency}
Efficiency category metrics are handled in this section. For this category, it must be remarked that following parameters have been considered:
\begin{itemize}\itemsep0pt
\item{\textbf{Tools:}} This category has been filled out basically through information existing on Benchmark studies about OpenNebula. Google, DuckDuckGo and a web browser (Firefox in this case) have been used.
\item{\textbf{Data Sources:}} Performance comparison between OpenStack and OpenNebula~\cite{BENCH00}.
\item{\textbf{Category Weight:}} As previously stated, for this category it has been considered a weight of 15\%.
\end{itemize}
Taking into account previous data, next table shows the final score of this category.
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{4cm} | p{3cm} | l | p{1.2cm} | p{1.2cm} | }
    \toprule
    \textbf{Metric} & \textbf{Score Obtained} & \textbf{Score} & \textbf{Metric Weight} & \textbf{Metric Score}\\
    \hline
    Performance Testing and Benchmark Reports available & Yes & 3 & 50\% & 1.5\\
    \hline
    Performance Tuning and Configuration & No & 1 & 50\% & 0.5\\
    \midrule
    \textbf{Unweighted rating} & \multicolumn{4}{c|}{\textbf{2}}\\
    \hline
    \textbf{Weighted rating (15\%)} & \multicolumn {4}{c|}{\textbf{0.3}}\\
    \bottomrule
    \end{tabular}
    \caption{Efficiency Score}
    \label{tab:effi_score}
  \end{center}
\end{table}

\subsection{Support}
Support category metrics is studied in this chapter. For this category, it must be remarked that following parameters have been considered:
\begin{itemize}\itemsep0pt
\item{\textbf{Tools:}} This category has been filled out basically through information existing on OpenNebula Open Source Project Web. Just a web browser (Firefox in this case) has been used.
\item{\textbf{Data Sources:}} OpenNebula information associated to Commercial Support~\cite{OPNEB01}.
\item{\textbf{Category Weight:}} As previously stated, for this category it has been considered a weight of 20\%.
\end{itemize}
Taking into account previous data, next table shows the final score of this category.
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{4cm} | p{3cm} | l | p{1.2cm} | p{1.2cm} | }
    \toprule
    \textbf{Metric} & \textbf{Score Obtained} & \textbf{Score} & \textbf{Metric Weight} & \textbf{Metric Score}\\
    \hline
    Company Support & Just one & 3 & 100\% & 3\\
    \midrule
    \textbf{Unweighted rating} & \multicolumn{4}{c|}{\textbf{3}}\\
    \hline
    \textbf{Weighted rating (20\%)} & \multicolumn {4}{c|}{\textbf{0.6}}\\
    \bottomrule
    \end{tabular}
    \caption{Support Score}
    \label{tab:supp_score}
  \end{center}
\end{table}

\subsection{Documentation}\label{sec:analysis_documentation}
Documentation category metrics are analyzed in this chapter, to measure the quality of the documenation, considered important from the invented role perspective. For this category, it must be remarked that following parameters have been considered:
\begin{itemize}\itemsep0pt
\item{\textbf{Data Sources:}} OpenNebula main source code repository, \textbf{One}, has been used to retrieve and analyze metrics. 
\item{\textbf{Tools:}} Tools used for this category have been basically \textbf{Git}, \textbf{cvsanaly2} and \textbf{MySQL Server}, to retrieve source code repository, classify it and analyze it respectively.
\begin{itemize}\itemsep0pt
\item{Git.} Git has been used to retrieve OpenNebula source code from previously described repository. The command issue for retrieval has been:
\shellcmd{git clone https://github.com/OpenNebula/one.git}
Previous command will create a directory, "one", where OpenNebula source code can be classified and analyzed.
\item{cvsanaly2.} cvsanaly2 tool has been used to classify source code existing on previous directory. Next command assumes that an empty database, called "cvsanaly\_one", already exists. Creation of a database with MySQL will be described later, on MySQL tool usage. cvsanaly2 command issued to perform source code classification and storage has been:
\shellcmd{cvsanaly2 --db-user=root --db-password=whatever --db-database=cvsanaly\_one --extensions=FileTypes}
It is remarkable that cvsanaly2 provides extensions, which are in the end plugins that allow to expand the possibilities of analysis. Extension "FileTypes" have been included, as this extension allow to classify the type of files by its extension.
\item{MySQL.} MySQL server is an important tool to calculate this metrics as well. On the one hand, "cvsanaly\_one" table has been created, through this commands:
\shellcmd{mysql -uroot -pwhatever}
Previous command gives access to MySQL server prompt. Next command allows creation of the database needed by cvsanaly2 to store the information appropriately sorted in different tables.
\mysqlcmd{create database cvsanaly\_one;}
Once the tables have been created and filled by "cvsanaly2" tool, queries must be performed in order to analyze the metrics needed for this category. It is not the objective of this document to include a deep study of the tables that cvsanaly.\\
\\
Queries used and information dumped by MySQL prompt for each of the metrics is shown below:
\shellcmd{mysql -uroot -pwhatever}
Previous command gives access to MySQL prompt. In order to use the database, next command has to be issued:
\shellcmd{use cvsanaly\_one}
To obtain when was documentation updated last time:
\mysqlcmd{select MAX(s.date) from files ff, scmlog s, file\_types f, actions a where s.id=a.commit\_id and f.file\_id=a.file\_id AND f.type='documentation' AND ff.file\_name LIKE '\%.txt' ORDER BY s.date;}
This query gives a resulting date equal to:
\begin{verbatim}
+---------------------+
| MAX(s.date)         |
+---------------------+
| 2013-11-28 15:11:52 |
+---------------------+
1 row in set (1.74 sec)
\end{verbatim}
Taking into account that the query was performed on 27th of December, 2013, resulting score is \"Last Month\".
Meanwhile, to obtain how many people contributed to documentation on last year, next query has been used:
\mysqlcmd{select DISTINCT(p.email) from files ff, scmlog s, file\_types f, people p, actions a where s.id=a.commit\_id and f.file\_id=a.file\_id AND f.type='documentation' AND ff.file\_name LIKE '\%.txt' AND YEAR(s.date)='2013' AND s.committer\_id=p.id; }
This query gives a resulting number equal to:
\begin{verbatim}
+--------------------------+
| email                    |
+--------------------------+
| jmelis@opennebula.org    |
| dmolina@opennebula.org   |
| tinova@opennebula.org    |
| rsmontero@opennebula.org |
| cmartin@opennebula.org   |
| jfontan@opennebula.org   |
+--------------------------+
6 rows in set (3.32 sec)
\end{verbatim}
\end{itemize}
Resulting score is, as demonstrated above, "5 to 10 people", and, in particular, 6.
\item{\textbf{Category Weight:}} As previously stated, for this category it has been considered a weight of 25\%.
\end{itemize}
Taking into account previous data, next table shows the final score of this category.
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{4cm} | p{3cm} | l | p{1.2cm} | p{1.2cm} | }
    \toprule
    \textbf{Metric} & \textbf{Score Obtained} & \textbf{Score} & \textbf{Metric Weight} & \textbf{Metric Score}\\
    \hline
    Documentation Update & Documentation updated last month & 4 & 50\% & 2\\
    \hline
    People Contributing Documentation on last year & 5 to 10 people & 4 & 50\% & 2\\
    \midrule
    \textbf{Unweighted rating} & \multicolumn{4}{c|}{\textbf{4}}\\
    \hline
    \textbf{Weighted rating (25\%)} & \multicolumn {4}{c|}{\textbf{1}}\\
    \bottomrule
    \end{tabular}
    \caption{Documentation Score}
    \label{tab:docu_score}
  \end{center}
\end{table}

\subsection{Community}
Last, but not least, Community category metrics are analyzed to measure the stability and community health. Nevertheless, this category has been considered the most important one. For this category, it must be remarked that following parameters have been considered:
\begin{itemize}\itemsep0pt
\item{\textbf{Data Sources:}} OpenNebula main source code repository, \textbf{One}, has been used to retrieve and analyze metrics. 
\item{\textbf{Tools:}} Tools used for this category have been basically \textbf{Git}, \textbf{cvsanaly2} and \textbf{MySQL Server}, to retrieve source code repository, classify it and analyze it respectively. 
\begin{itemize}\itemsep0pt
\item{Git.} Git has been used to retrieve OpenNebula source code from previously described repository. Usage is exactly the same as described in~\nameref{sec:analysis_documentation}.
\item{cvsanaly2.} cvsanaly2 tool has been used to classify source code existing on previous directory. Usage is exactly the same as described in~\nameref{sec:analysis_documentation}.
\item{MySQL.} MySQL server is an important tool to calculate this metrics as well. Apart from database creation, described on previous section~\nameref{sec:analysis_documentation}, MySQL has been used in order to obtain the metrics need to score the project on this category. Queries used and information dumped by MySQL prompt for each of the metrics is shown below:
\shellcmd{mysql -uroot -pwhatever}
Previous command gives access to MySQL prompt. In order to use the database, next command has to be issued:
\shellcmd{use cvsanaly\_one}
To obtain the number of committers per developer on last month (last complete month was November), first the total number of committers must be issued:
\mysqlcmd{select COUNT(DISTINCT(p.email)) as "Number of Commiters" from scmlog s, people p WHERE YEAR(s.date)='2013' AND MONTH(s.date)='11' AND s.committer\_id=p.id;}
This query gives a resulting number equal to:
\begin{verbatim}
+---------------------+
| Number of Commiters |
+---------------------+
|                   6 |
+---------------------+
1 row in set (0.11 sec)
\end{verbatim}
On the other hand, the total number of commits must be calculated. In particular, it can be done with next query:
\mysqlcmd{select COUNT(s.id) as "Number of Commits" from scmlog s, people p WHERE YEAR(s.date)='2013' AND MONTH(s.date)='11' AND s.committer\_id=p.id;}
This query gives a resulting number equal to:
\begin{verbatim}
+-------------------+
| Number of Commits |
+-------------------+
|               315 |
+-------------------+
1 row in set (0.04 sec)
\end{verbatim}
Resulting score is 315/6 = 52.5, which is in the end \"20 or more commits / developer\".
Meanwhile, to obtain the territoriality of files, first the total number of files must be calculated:
\mysqlcmd{SELECT COUNT(DISTINCT(file\_path)) AS "File Number" FROM file\_links;}
This query gives a resulting number equal to:
\begin{verbatim}
+-------------+
| File Number |
+-------------+
|        4215 |
+-------------+
1 row in set (3.05 sec)
\end{verbatim}
On the other hand, the number of people who modified just one file can be calculated with next command:
\mysqlcmd{SELECT f.file\_path, COUNT(p.email) AS "People touching file" FROM scmlog as s, people as p, file\_links as f WHERE f.commit\_id=s.id AND s.committer\_id=p.id GROUP BY f.file\_path HAVING COUNT(p.email)=1;}
This query gives huge result with all the files being touched by just one person, giving a resulting number equal to:
\begin{verbatim}
+--------------------------------+----------------------+
| file_path                      | People touching file |
+--------------------------------+----------------------+
| apptools                       |                    1 |
| apptools/install.sh            |                    1 |
| ...                            |                  ... |
| sunstone/routes/appenv.rb      |                    1 |
| sunstone/routes                |                    1 |
+--------------------------------+----------------------+
2230 rows in set (0.00 sec)

\end{verbatim}
Resulting score is, as demonstrated above 2230/4215 = 0.529, which gives an score of "more than 50\% of the files".
Last, but not least, it must be calculated how community has grown or abated on last year. To do so, the number of committers on 2012 has been calculated with next query:
\mysqlcmd{select DISTINCT(p.email) as "Number of Commits" from scmlog s, people p WHERE YEAR(s.date)='2012' AND s.committer\_id=p.id;}
This query gives a resulting number equal to:
\begin{verbatim}
+--------------------------+
| Committers               |
+--------------------------+
| rubensm@dacya.ucm.es     |
| j.melis@fdi.ucm.es       |
| jfontan@gmail.com        |
| cmartins@fdi.ucm.es      |
| tinova79@gmail.com       |
| danmolin@fdi.ucm.es      |
| support@c12g.com         |
| hsanjuan@opennebula.org  |
| rsmontero@opennebula.org |
| ruben@pc-ruben.(none)    |
| jmelis@opennebula.org    |
| jfontan@opennebula.org   |
| tinova@opennebula.org    |
| dmolina@opennebula.org   |
| cmartin@opennebula.org   |
| hector@convivencial.org  |
+--------------------------+
16 rows in set (0.03 sec)
\end{verbatim}
Later, the number of commiters in 2013 has been calculated by issuing next query:
\mysqlcmd{select DISTINCT(p.email) as "Number of Commits" from scmlog s, people p WHERE YEAR(s.date)='2013' AND s.committer\_id=p.id;}
This query gives a resulting number equal to:
\begin{verbatim}
+--------------------------+
| Committers               |
+--------------------------+
| jfontan@gmail.com        |
| tinova79@gmail.com       |
| rsmontero@opennebula.org |
| jmelis@opennebula.org    |
| jfontan@opennebula.org   |
| tinova@opennebula.org    |
| dmolina@opennebula.org   |
| cmartin@opennebula.org   |
| dmolina@c12g.com         |
+--------------------------+
9 rows in set (0.04 sec)
\end{verbatim}
This information is not enough, as, on this small number of results, it can be observed that some committers are repeated. Indeed, for each table, it can be asserted than on 2012 there were 9 unique committers. Meanwhile, on 2013, there were 6 unique committers. After fixing the results, it can be ensured that resulting score is, as demonstrated above, is a decrease of 3/9 =0.33 (33\%), what means a "decrease of more than 25\%".
\end{itemize}
\item{\textbf{Category Weight:}} As previously stated, for this category it has been considered a weight of 30\%.
\end{itemize}
Taking into account previous data, next table shows the final score of this category.
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{4cm} | p{3cm} | l | p{1.2cm} | p{1.2cm} | }
    \toprule
    \textbf{Metric} & \textbf{Score Obtained} & \textbf{Score} & \textbf{Metric Weight} & \textbf{Metric Score}\\
    \hline
    Mean commits / developer last month & 20 or more / developer & 5 & 33\% & 1.67\\
    \hline
    Percent of files only modified by one developer & \textgreater50\% & 1 & 33\% & 0.33\\
    \hline
    Community growth & \textless-25\% & 1 & 33\% & 0.33\\
    \midrule
    \textbf{Unweighted rating} & \multicolumn{4}{c|}{\textbf{2.33}}\\
    \hline
    \textbf{Weighted rating (30\%)} & \multicolumn {4}{c|}{\textbf{0.7}}\\
    \bottomrule
    \end{tabular}
    \caption{Community Score}
    \label{tab:comm_score}
  \end{center}
\end{table}

\subsection{Final Score}\label{sec:final_score}
With data existing on the previous tables, where partial scores for each category and weighted rating have been calculated, give the possibility to calculate the final total score for OpenNebula.\\
\\
Final score is just calculated by adding the weighted rating of each of the category, what gives a final result of 3.02, as shown summarized in next table:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{3cm} | p{2.5cm} | p{1.5cm} | p{2cm} | }
    \toprule
    \textbf{Category} & \textbf{Unweighted rating} & \textbf{Weigth} & \textbf{Weighted Score}\\
    \hline
    Functionality & 4.2 & 10\% & 0.42\\
    \hline
    Efficiency & 2 & 15\% & 0.3\\
    \hline
    Support & 3 & 20\% & 0.6\\
    \hline
    Documentation & 4 & 25\% & 1\\
    \hline
    Community & 2.33 & 30\% & 0.7\\
    \midrule
    \textbf{Final Score} & \multicolumn {3}{c|}{\textbf{3.02}}\\
    \bottomrule
    \end{tabular}
    \caption{OpenNebula Score}
    \label{tab:final_score}
  \end{center}
\end{table}
Once the Final Score for OpenNebula has been obtained, one question appears: \textbf{Is OpenNebula a good quality project? There is no answer to this question}. OpenBRR does not define to which level a project is a quality project, as it just allows comparing between projects of the same nature for a evaluator to take a final decission.\\
\\
On next section, taking advantage of the scores obtained to the same Quality Model from other Cloud Computing Open Source Projects, in particular CloudStack, Eucalyptus and OpenStack, and adjusting the weights of them to be the the same as the ones used for calculating OpenNebula score, a calculation of the best project fitting the necessities of the role will be performed.
\section{Results} \label{sec:results}
On previous section~\nameref{sec:analysis}, the calculation of the quality score of OpenNebula Open Sour ce Project has been carried out. However, this score is only valid to compare the project against other similar nature projects in order to take a decission on which project fits better for the necessities identified by the invented role described in subsection~\nameref{sec:roledesc}.\\
\\
In parallel to the redaction of this final assignment, other lectures have performed the evaluation of CloudStack, Eucalyptus and OpenStack Open Source Projects. Spreadsheets developed by them can be obtained on next repository:
\\
\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev.git}\\
\\
In particular, the Quality Model Spreadsheets can be downloaded, respectively, on next paths:
\begin{itemize}\itemsep0pt
\item{"spreadsheet/CloudStack/CloudStack\_BRR\_ProjEval\_2013.ods"}
\item{"spreadsheet/eucalyptus/BRR\_ProjEval\_eucalyptus.ods"}
\item{"spreadsheet/OpenStack/OpenStack\_BRR\_ProjEval\_2013.ods"}
\end{itemize}
or directly through next urls:
\begin{itemize}\itemsep0pt
\item{\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev/blob/master/spreadsheet/CloudStack/CloudStack\_BRR\_ProjEval\_2013.ods}}
\item{\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev/blob/master/spreadsheet/eucalyptus/BRR\_ProjEval\_eucalyptus.ods}}
\item{\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev/blob/master/spreadsheet/OpenStack/OpenStack\_BRR\_ProjEval\_2013.ods}}
\end{itemize}
Previous models contain an analysis of the rest of the Cloud Computing Open Source Projects, similar to the one performed . However, they contain weights adapted to what other students' invented roles have considered appropriate.\\
\\
\textbf{For this reason, the models have been reused in order to calculate the scores of each of these other projects}, but taking into account the invented role described previously on section~\nameref{sec:roledesc}, which, in turn, means considering the weights described in~\nameref{sec:weights}.\\
\\
The score of each of the rest of the projects is shown in next subsections, on a summarized way similar to the Final Score described in subsection\label{sec:final_score}. Besides this, a comparison table showing a top down projects classification, always considering the Quality Model proposed in subsection~\nameref{sec:quality_model}.\\
\\
It must be remarked that information existing on next subsections can also be obtained through next repository:
\\
\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev.git}\\
\\
In particular, the Quality Model Spreadsheets can be downloaded, respectively, on next paths:
\begin{itemize}\itemsep0pt
\item{"spreadsheet/OpenNebula/CloudStack\_BRR\_ProjEval\_2013.ods"}
\item{"spreadsheet/OpenNebula/BRR\_ProjEval\_eucalyptus.ods"}
\item{"spreadsheet/OpenNebula/OpenStack\_BRR\_ProjEval\_2013.ods"}
\end{itemize}
or directly through next urls:
\begin{itemize}\itemsep0pt
\item{\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev/blob/master/spreadsheet/OpenNebula/CloudStack\_BRR\_ProjEval\_2013.ods}}
\item{\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev/blob/master/spreadsheet/OpenNebula/BRR\_ProjEval\_eucalyptus.ods}}
\item{\url{https://github.com/MSWL-PROJ-EV-2013-2014/projev/blob/master/spreadsheet/OpenNebula/OpenStack\_BRR\_ProjEval\_2013.ods}}
The reason for each of the projects to be contained in \"OpenNebula\" folder is simply that they follow the weights proposed for OpenNebula project.
\end{itemize}

\subsection{CloudStack}
This section contemplates the final score of CloudStack Open Source Project. Final score is just calculated by adding the weighted rating of each of the categories, what gives a final result of 3.571, as shown summarized in next table:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{3cm} | p{2.5cm} | p{1.5cm} | p{2cm} | }
    \toprule
    \textbf{Category} & \textbf{Unweighted rating} & \textbf{Weigth} & \textbf{Weighted Score}\\
    \hline
    Functionality & 3.8 & 10\% & 0.38\\
    \hline
    Efficiency & 3 & 15\% & 0.45\\
    \hline
    Support & 3 & 20\% & 0.6\\
    \hline
    Documentation & 5 & 25\% & 1.25\\
    \hline
    Community & 2.97 & 30\% & 0.891\\
    \midrule
    \textbf{Final Score} & \multicolumn {3}{c|}{\textbf{3.571}}\\
    \bottomrule
    \end{tabular}
    \caption{CloudStack Score}
    \label{tab:cloudstack_score}
  \end{center}
\end{table}

\subsection{Eucalyptus}
This section contemplates the final score of Eucalyptus Open Source Project. Final score is just calculated by adding the weighted rating of each of the categories, what gives a final result of 3.43, as shown summarized in next table:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{3cm} | p{2.5cm} | p{1.5cm} | p{2cm} | }
    \toprule
    \textbf{Category} & \textbf{Unweighted rating} & \textbf{Weigth} & \textbf{Weighted Score}\\
    \hline
    Functionality & 4.6 & 10\% & 0.46\\
    \hline
    Efficiency & 3 & 15\% & 0.45\\
    \hline
    Support & 3 & 20\% & 0.6\\
    \hline
    Documentation & 4.5 & 25\% & 1.13\\
    \hline
    Community & 2.64 & 30\% & 0.79\\
    \midrule
    \textbf{Final Score} & \multicolumn {3}{c|}{\textbf{3.43}}\\
    \bottomrule
    \end{tabular}
    \caption{Eucalyptus Score}
    \label{tab:eucalyptus_score}
  \end{center}
\end{table}

\subsection{OpenStack}
Finally, the final score of OpenStack Open Source Project. Final score is just calculated by adding the weighted rating of each of the categories, what gives a final result of 3.31, as shown summarized in next table:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{3cm} | p{2.5cm} | p{1.5cm} | p{2cm} | }
    \toprule
    \textbf{Category} & \textbf{Unweighted rating} & \textbf{Weigth} & \textbf{Weighted Score}\\
    \hline
    Functionality & 3.4 & 10\% & 0.34\\
    \hline
    Efficiency & 3 & 15\% & 0.45\\
    \hline
    Support & 5 & 20\% & 1\\
    \hline
    Documentation & 4.5 & 25\% & 1.13\\
    \hline
    Community & 1.32 & 30\% & 0.4\\
    \midrule
    \textbf{Final Score} & \multicolumn {3}{c|}{\textbf{3.31}}\\
    \bottomrule
    \end{tabular}
    \caption{OpenStack Score}
    \label{tab:openstack_score}
  \end{center}
\end{table}

\subsection{Cloud Computing Open Source Projects Comparison}
Last, but not least, it is important to set a top-down table, ordered by weighted final score, with the Quality Model decided. Next table shows project classification depending on the Final Score, as well as the weighted score obtained for each category:
\begin{table}[H]
  \begin{center}
    \begin{tabular}{ | p{2.5cm} | p{1cm} | p{2.2cm} | p{1.8cm} | p{1.7cm} | p{2.4cm} | p{1.9cm} | }
    \toprule
    Project & \textbf{Final Score} & Functionality & Efficiency & Support & Documentation & Community\\
    \hline
    CloudStack & \textbf{3.57} & 0.38 & 0.45 & 0.6 & 1.25 & 0.891\\
    \hline
    Eucalyptus & \textbf{3.43} & 0.46 & 0.45 & 0.6 & 1.13 & 0.79\\
    \hline
    OpenStack & \textbf{3.31} & 0.34 & 0.45 & 1 & 1.13 & 0.4\\
    \hline
    OpenNebula & \textbf{3.02} & 0.42 & 0.3 & 0.6 & 1 & 0.7\\
    \midrule
    \textbf{Winner Project} & \multicolumn {6}{c|}{\textbf{CloudStack}}\\
    \bottomrule
    \end{tabular}
    \caption{Comparison Table}
    \label{tab:comparison_table}
  \end{center}
\end{table}
For this reason, it can be ensured that, according to the Quality Model and the weights specified, \textbf{CloudStack is the most appropriate Cloud Computing Open Source Project}, at least among the set of projects which have been analyzed.

\section{Conclussion} \label{sec:conclussion}
Contrary to what may appear initially, where OpenStack seems to be the most important Cloud Stack Computing Open Source Project, the final conclussion of this document is clear: \textbf{CloudStack is the most appropriate Cloud Computing Open Source Project}, at least, for a self-defined Quality Model and taking into account those factors considered more important for an invented role acting as evaluator. However, special attention has to be paid before taking the final decission. At least, some inspection on the results of the weighted scores for each category against this project competitors must be analyzed.\\
\\
At a first glance, it could happen that the project was much better in less important categories, such as Functionality, Efficiency or even Support, compared to CloudStack competitors, but nothing could be further from the truth. CloudStack is the \textbf{best project} in what has been determined the most important categories for the invented role, namely, \textbf{Documentation and Community}.\\
\\
Besides this, on the rest of categories, CloudStack is never the worst case, at least on a unique way. For Functionality category, CloudStack is, at least, better than OpenStack, although worst than OpenNebula and Eucalyptus. Meanwhile, for Efficiency category, CloudStack equals in first place with Eucalyptus and OpenStack, improving OpenNebula. Support category is not the strength of CloudStack, but at least it seems that its score equals Eucalyptus and OpenNebula.\\
\\
Considering previous information, there is no doubt that \textbf{CloudStack Open Source Project is the one fitting best the requirements of the evaluator}.\\
\\
This document is an entry point to the different steps that must be followed in order to determine the best option when selecting a particular Open Source project, and in particular:
\begin{enumerate}
\item{\textbf{Identifying the set of candidate projects.}} Select a group of Open Source Projects among the different possibilities by considering the nature of the product that must be acquired, together with the information existing on specialized magazines, scientific papers, blogs and other Internet information sources in order to build a set of candidate projects to be evaluated.
\item{\textbf{Determining a Quality Model.}} It can be based on existing ones, as described in section~\nameref{sec:oss_quality_models}, or a self-defined/adapted version to fit better into the requirements of the company. It is recommended to set a Quality Model based on GQM (Goal/Question/Metric) model, by setting Goals, rasing Question and obtaining Metrics that allow answering them.
\item{\textbf{Weighting the different categories.}} Depending on the type of Quality Model selected, it could be possible that a classification in importance of the different categories existing on the model must be carried out, as described in subsection~\nameref{sec:weights}.
\item{\textbf{Analyzing and Classifying candidate projects.}} Using a particular set of tools, applied to different sets of data sources, an analysis of the candidate projects should be performed. This analysis must determine as result a final top-down classification of the different projects, for the evaluator to select the most appropriate project for the necessities proposed.
\item{\textbf{Final decission.}} Before takign the final decission, some other questions must be asked: Is the project a good option for my necessities? Does the project have special weakness compared to other projects? And if so, can they suppose a problem for the company/organization?. Are there projects that, although being slightly worst in the Quality Model score result, are, somehow, more balanced, having fewer weaknesses?
\end{enumerate}
Taking a decission on which Open Source Project must be acquired is normally a hard thing. This document aims to be an entry point to follow some very basic steps for facilitating this task, and minimize the risk associated to facing a wrong selection.
\bibliographystyle{unsrt}
\bibliography{mybib}{}
\end{document}
